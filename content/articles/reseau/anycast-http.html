---
title: HTTP et anycast
author: Renaud Chaput
---

      <h1>Objectif</h1>
<p>Réaliser une architecture sécurisée pour un service HTTP, avec les contraintes :</p>
<ul>
<li>Multisites (deux ou plus)</li>
<li> Pas d'actif/passif (tous les sites doivent recevoir le traffic)</li>
<li> Géolocalisation (les visiteurs doivent arriver sur le site le plus "proche")</li>
<li> Pas de lien niveau 2 entre les sites</li>
<li> Réalisable avec de l'open source</li>
</ul>
<h1>Solutions possibles</h1>
<h2>Appliances de loadbalancing</h2>
<p>Plusieurs solutions ont été étudiées. La première est l'utilisation d'appliances de loadbalancing, capables de failover et de répartition géographique. Ce genre de solutions est (généralement) très cher, et n'est pas forcément scalable. De plus, toutes ne gèrent pas le failover multisites sans lien L2 entre eux.</p>
<h2>Keepalived / CARP</h2>
<p>Une seconde solution, classique, est l'utilisation de Keepalived (ou CARP, ou Hearthbeat, ou ...). Ces solutions sont éprouvées et ne nécessitent aucun matériel spécifique, et peuvent être mises en place en utilisant uniquement des logiciels libres. Par contre, il est nécessaire que tous les sites géographiques soient sur le même lien niveau 2, ce qui n'est pas possible dans notre cas.</p>
<h2>GeoDNS</h2>
<p>Une solution couramment utilisée est le GeoDNS. Le principe est simple : quand une requête DNS arrive sur le serveur autoritaire de la zone, celui-ci géolocalise l'IP source de la requête, et renvoie l'IP du site géographique le plus proche. Cette méthode fonctionne relativement bien et est très répandue, mais souffre de deux défauts.<br />
Premièrement, elle n'est pas précise. En effet, l'IP source de la requête DNS n'est que très rarement celle du client. Celui-ci utilise le plus souvent les serveurs DNS de son FAI, ou des serveurs DNS publics (type OpenDNS ou Google DNS). Ces serveurs peuvent être situés géographiquement loin de l'utilisateur, ou bien utiliser un cache mondial qui fausse totalement les informations de GeoDNS.<br />
De plus, il n'est pas possible de faire du failover réactif avec ce système. Il n'est pas recommandé d'utiliser un Time-To-Live inférieur à 5 minutes, ce qui devient le temps minimal de réponse de notre système. De plus, certains resolver DNS ignorent le TTL, et peuvent mettre 48 heures à se mettre à jour.</p>
<h2>Anycast</h2>
<p>Une dernière solution est possible : l'<a href="http://en.wikipedia.org/wiki/Anycast">anycast</a>. Cette technique est principalement utilisée pour les DNS. En effet, comme une IP peut être annoncée par plusieurs machines, il n'est pas garantit que tous les paquets envoyés par un client arriveront sur la même machine. le DNS utilisant des petits paquets UDP (mode non connecté), cela ne pose pas de problème, une requête DNS n'étant la plupart du temps qu'un paquet émis, et un reçu. Par contre, pour l'HTTP, cela peut être beaucoup plus gênant. En effet, HTTP utilise le protocole TCP, qui est un protocole connecté. La connection TCP nécessite plusieurs paquets, qui doivent avoir une même machine source et destination.</p>
<p>Au final, c'est une solution basée sur Anycast qui va être utilisée, mais en essayant d'éviter les instabilités de connexion exposées ci dessus.</p>
<h1>Principe</h1>
<p>La solution mise en place se base sur le même principe que Anycast : le failover de l'IP de service (<i>ip-a</i> dans la suite) va se faire via le routage.<br />
Par contre, afin de garantir une stabilité dans la machine qui reçoit la connexion, il ne va pas falloir que les différentes routes vers cette IP aient le même poids.<br />
<u>Note</u> : par convention ici, la route choisie est celle ayant le poids le plus faible</p>
<p>Nous allons avoir deux routes pour <i>ip-a</i> :</p>
<ul>
<li>via LB1, avec une priorité faible</li>
<li>via LB2, avec une priorité élevée</li>
</ul>
<p>Ainsi, tous les paquets à destination de <i>ip-a</i> arriveront sur LB1. Si celui-ci n'est plus accessible, la route doit être supprimée (nous verrons comment par la suite), et tous les paquets à destination de cette IP seront routés (et pris en charge) par LB2.</p>
<p>Avec cette approche, nous perdons le loadbalancing entre les deux sites. Mais il est possible de le conserver en mettant en place une seconde IP pour le service HTTP, <i>ip-b</i>, qui a une route de priorité élevée vers LB1, et de priorité faible sur LB2. Ainsi, la route préférée sera celle vers LB2, et l'IP basculera vers LB1 en cas de problème sur LB2.</p>
<p>Il suffit ensuite de diriger les visiteurs vers ces deux IPs, en utilisant par exemple du <a href="http://en.wikipedia.org/wiki/Round_robin_DNS">Round Robin DNS</a>. On peut également utiliser un serveur DNS autoritaire qui renvoie aléatoirement une des deux IPs lorsqu'il est interrogé. Ces IPs peuvent être mises en cache au niveau des resolvers sans problèmes, vu que les deux servent le même service (sauf si LB1 et LB2 sont down en même temps, évidement).</p>
<p>Cette approche permet de scaler assez facilement en taille. Il est possible d'ajouter facilement des loadbalancers. Si ceux-ci sont dans le même MAN, il faut rajouter une IP pour le service, la configurer sur tous les loadbalancers, et on se retrouve avec 3 routes pour une IP. Si l'on a une situation avec 2 sites sur un continent, et 2 sites sur l'autre, on peut faire avec deux IPs seulement : les deux IPs sont annoncées sur chaque continent, avec une configuration par continent identique à celle présentée au dessus. Si un site d'un continent tombe, l'autre site du continent prend le relais. Si les deux sites du continent tombent, les visiteurs vont arriver sur l'autre continent (fonctionnement moins optimal, mais le service est toujours présent). Par contre, il faut que BGP reconverge dans ce cas, ce qui peut ajouter un délai.</p>
<p>Il n'est pas possible d'effectuer une répartition de charge pondérée et précise entre les différents sites, la répartition se faisant au niveau DNS. Il est possible de le faire au niveau DNS (<a href="http://code.google.com/p/ruby-pdns/wiki/RecipeWeightedRoundRobin">exemple</a>), mais cela ne sera pas toujours très précis : si une majorité des clients est situé derrière peu de resolvers, la répartition ne sera pas celle voulue.</p>
<h1>Matériel</h1>
<p>Cette partie est à adapter selon vos besoin, votre volumétrie et l'existant. Pour cet article, j'utilise :</p>
<ul>
<li>Deux sites géographiques, accessibles via un interlan Layer 3 (routé)</li>
<li>Deux routeurs supportant le BGP. J'utilise des Cisco, mais un PC avec un Quagga suffit. Dans cet article, je considère que la configuration BGP est fonctionnelle, avec une session iBGP entre eux.</li>
<li>Sur chaque site géographique :
<ul>
<li>Un loadbalancer HTTP. J'utilise un serveur avec haproxy, mais le choix est libre. Si c'est un UNIX, cela simplifiera les choses par la suite</li>
<li>Un pool de serveurs HTTP</li>
</ul>
</li>
</ul>
<p>Un petit schéma (n'inclut pas les switchs, et les élements de redondance réseau) :</p>
<div align="center"><img src="/images/renchap/schema_general.png" /></div>

